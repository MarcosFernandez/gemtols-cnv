#!/usr/bin/env
"""Pipeline utilities"""
import gem
import json
import logging
import os
import errno
import signal
import traceback

from gem.utils import Timer
import gem.gemtools as gt
import gem.filter
import gem.utils
import gem.cnvReport
from gem.pipeline import *
from types import *

class BwaMappingStep(PipelineStep):
    """BWa MEM Mapping step"""
    
    def files(self):
        """Return the output files generated by this step.
        By default one .map output file is generated
        """
        if self._files is None:
            self._files = []

            nameFiles = self.pipeline.create_file_name(self.name,sub_directory=self.name,
                                                       name_suffix=self.name_suffix,file_suffix=self.file_suffix,
                                                       final=self.final)
            self._files.append(nameFiles)
                
        return self._files    
    
    def run(self):
        cfg = self.configuration
 
        gem.bwaMapper(self.pipeline.input,cfg["bwa_reference"],output=self.files()[0],
                                    threads=cfg["threads"],name=self.pipeline.name,compress=self._compress(),tmp=cfg["tmp_folder"])
                                    
class MergeMappingStep(PipelineStep):
    """ Merge Mapping steps """
    def files(self):
        """Return the output files generated by this step."""
        if self._files is None:
            self._files = []
            merged_bam = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="merged.bam")
            self._files.append(merged_bam)
            
        return self._files   
        
    def run(self):
        cfg = self.configuration
        gem.mergeBams(input=self.pipeline.input,output=self.files()[0],threads=cfg["threads"])

   
class MarkDuplicatesStep(PipelineStep):
    """Remove PCR Duplicates step"""
    
    def files(self):
        """Return the output files generated by this step."""
        if self._files is None:
            self._files = []
            
            metrics_out = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="metrics")
            rmdup_out = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="rmdup.bam")
            self._files.append(metrics_out)
            self._files.append(rmdup_out)
            
        return self._files    
    
    def run(self):
        cfg = self.configuration
       
        input = ""
        if self.pipeline.merge_bams:
            input += self._input(True)
        else:
            input += self.pipeline.input[0]
        gem.markDuplicates(input,picard_tools_path=cfg["picard_path"],java_heap=cfg["java_heap"],
                               tmp_folder=cfg["tmp_folder"],output=self.files())
                               
                                     
class BamToFastq(PipelineStep):
    """Bam to Fastq Step"""

    def files(self):
        """Return the output files generated by this step."""
        if self._files is None:
            self._files = []
            distinguishPE = ""
            
            if self.pipeline.single_end == False:
                distinguishPE +="#"
            
            fastq_out = self.pipeline.create_file_name(distinguishPE, sub_directory=self.name, file_suffix="fq")
            
            self._files.append(fastq_out)
            
        return self._files 
   
    def run(self):
        inputs = self.pipeline.input
        gem.bamToFastq(inputs,output=self._final_output())

    def is_done(self):
        """Return true if this step is done and
        does not need execution

        Checks if all files exists
        """
        return self.checkFileExistance(self.files())
            
    def checkFileExistance(self,listFiles):
        '''Checks for the file existance in a set of files'''
        for file in listFiles:
            if self.pipeline.single_end:
                if not os.path.exists(file):
                    return False
            else:
                pairOne = file.replace("#","_1")
                pairTwo = file.replace("#","_2")
                if not os.path.exists(pairOne):
                    return False
                if not os.path.exists(pairTwo):
                    return False 
        return True
    
    def getAllFiles(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on mapping BaseMappingStep!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0]
        
    def cleanup(self, force=False):
        if force or (not self.final and self.pipeline.remove_temp):
            for f in self.files():
                if self.pipeline.single_end:
                    if not os.path.exists(f):
                        logging.gemtools.debug("Remove temporary file %s" % f)
                        os.remove(f)
                else:
                    pairOne = f.replace("#","_1")
                    pairTwo = f.replace("#","_2")
                    if not os.path.exists(f):
                        logging.gemtools.debug("Remove temporary file %s" % pairOne)
                        logging.gemtools.debug("Remove temporary file %s" % pairTwo)
                        os.remove(pairOne)
                        os.remove(pairTwo)

class FragmentReadsStep(PipelineStep):
    """ Fragment reads """  
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
  
            if self.pipeline.single_end:
                for fragment in range(1,self.pipeline.split_times+1):
                    self.prefix = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="")                  
                    breakFile = self.prefix + ".part-" + str(fragment)
                    self._files.append(breakFile)
            else:
                for fragment in range(1,self.pipeline.split_times+1):
                    self.prefix = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="")                  
                    breakFile_1 = self.prefix + ".1.part-" + str(fragment) 
                    breakFile_2 = self.prefix + ".2.part-" + str(fragment) 
                    self._files.append(breakFile_1)
                    self._files.append(breakFile_2)
          
        return self._files
                
    def run(self): 
        cfg = self.configuration

        inputs = []
        if self.pipeline.single_end:
            if len(self.dependencies) == 0: 
                inputs.append(self.pipeline.input[0])
            else:
                inputs.append(self._input(True))
        else:
            if len(self.dependencies) == 0:       
                inputs.append(self.pipeline.input[0])
                inputs.append(self.pipeline.input[1])  
            else:
                inputs.append(self._input(True).replace("#","_1"))
                inputs.append(self._input(True).replace("#","_2"))  
            
        gem.fragmentFastq(inputs,output=self.files(),split_times=cfg['split_times'],gz=cfg['gzipped'],prefix=self.prefix,threads=cfg['threads'])
    
    def is_done(self):
        """Return true if this step is done and
        does not need execution

        Checks if all files exists
        """
        for f in self.files():
            if not os.path.exists(f):
                return False
        return True
        

       
class ChopStep(PipelineStep): 
    """ Fragment and Chop reads to 36bp """  
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
  
            if self.pipeline.single_end:
                for fragment in range(1,self.pipeline.split_times+1):
                    chopFile = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="part-" + str(fragment) + ".fq")   
                    self._files.append(chopFile)
            else:
                for fragment in range(1,self.pipeline.split_times+1):
                    chopFile_1 = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="part-" + str(fragment) + ".1.fq")   
                    chopFile_2 = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="part-" + str(fragment) + ".2.fq")   
                    self._files.append(chopFile_1)
                    self._files.append(chopFile_2)   
        
        return self._files
                
    def run(self): 
        cfg = self.configuration

        gem.chopFQ(self._input()[0],output=self.files(),first_position=cfg['first_position'],\
                   split_times=cfg['split_times'],kmer_len=cfg['kmer_length'],slide=cfg['windowing'],threads=cfg['threads'])
    
    def is_done(self):
        """Return true if this step is done and
        does not need execution

        Checks if all files exists
        """
        for f in self.files():
            if not os.path.exists(f):
                return False
        return True
        
    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on mapping FragmentReadsStep!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0]
    
    
class CnvMapping(PipelineStep):
    """ Gem Mapping of Chopped Reads """

    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            sam_file = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="map")
            self._files.append(sam_file)
            
        return self._files


    def run(self):
        """Run gem.cnvMapper 
        Arguments--
        input: fastq file to be mapped
        output: output filer or strem to store mapping result
        """
        cfg = self.configuration
        gem.cnvMapper(
            self.pipeline.input,
            cfg["index"],
            self.files()[0],
            mismatch_alphabet=cfg["mismatch_alphabet"],
            mismatches=cfg["mismatches"],
            max_edit_distance=cfg["max_edit_distance"],       
            strata_after_best=cfg["strata_after_best"],        
            max_decoded_matches=cfg["max_decoded_matches"],
            min_decoded_strata=cfg["min_decoded_strata"],
            threads_number=cfg["genome_threads"]   
        )
        
        
    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on chop reads analysis!")
        return self.pipeline.steps[self.dependencies[-1]].files()
                
    def is_done(self):
        """Return true if this step is done and
        does not need execution

        Checks if all files exists
        """
        for f in self.files():
            if not os.path.exists(f):
                return False
        return True
            
    def getAllFiles(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on Chop Reads Step!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0] 

class MappingStatsStep(PipelineStep):
    """Create mapping stats file"""

    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            s = self.name_suffix if self.name_suffix is not None else ""
            seTxt = self.pipeline.create_file_name(self.name, sub_directory=self.name, name_suffix="%s.stats" % s, file_suffix="txt", 
                                               final=self.final,multi_fastq = self.pipeline.multiFastq)
            seJson = self.pipeline.create_file_name(self.name, sub_directory=self.name, name_suffix="%s.stats" % s, file_suffix="json",
                                               final=self.final,multi_fastq = self.pipeline.multiFastq)
            self._files.append(seTxt)
            self._files.append(seJson)
            
        return self._files
        
    def run(self):
        outputs = self.files()
        infile = self._input()
        gem.statsCnv(infile[0], output=outputs[0], json_output=outputs[1],paired=False,threads=self.pipeline.genome_threads,raw=False)
            
          
    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on cnv mapping analysis!")
        return self.pipeline.steps[self.dependencies[-1]].files()
        
        
    def is_done(self):
        """Return true if this step is done and
        does not need execution

        Checks if all files exists
        """
        for f in self.files():
            if not os.path.exists(f):
                return False
        return True
              
    def getAllFiles(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on cnv mapping analysis!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0] 
        

class MapToSamStep(PipelineStep):
    """ From map file to sam file """
    
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            sam = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="sam.gz",multi_fastq = self.pipeline.multiFastq)
            self._files.append(sam)
            
        return self._files
        
    def run(self):
        cfg = self.configuration
        gem.mapToSam(self._input()[0],cfg["index"],self.files()[0],self.name,cfg["genome_threads"])
        
    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on chop reads analysis!")
        return self.pipeline.steps[self.dependencies[-1]].files()
    
    def is_done(self):
        """Return true if this step is done and
        does not need execution

        Checks if all files exists
        """
        for f in self.files():
            if not os.path.exists(f):
                return False
        return True
            
    def getAllFiles(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on cnv mapping analysis!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0] 
        
class MrCanavarRDStep(PipelineStep):
    """Compute Read Depth MrCanavar Step """

    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            depth_file = self.pipeline.create_file_name("mrcanavar", sub_directory="mrcanavar", file_suffix="depth")
            self._files.append(depth_file)
            
        return self._files

        
    def run(self):
        cfg = self.configuration
        depth_file = self.files()[0]
        gem.mrCanavarReadDepth(self.pipeline.input, cfg["conf_file"], depth_file,cfg["maps_gzipped"])
    
class MrCanavarCallsStep(PipelineStep):
    """Perform copy number calls MrCanavar Step """

    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            calls_file = self.pipeline.create_file_name("mrcanavar", sub_directory="mrcanavar", file_suffix="calls")
            self._files.append(calls_file)
            
        return self._files

        
    def run(self):
        cfg = self.configuration
        calls_file = self.files()[0]
     
        gem.mrCanavarCalls(
            self._input(True),
            conf_file = cfg["conf_file"],
            calls_file = calls_file
        )
    
    
class CopyNumberDistributionStep(PipelineStep):
    """Copy Number Distribution Step """
    
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            
            cn_rdata_file = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="copy_number_distribution.RData")
            cutoffs_file = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="specific_cutoffs.txt")
            blorente_plot = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="controlRegions_density_plot.png")           
            blorente_ctrl_regions_dstrb = self.pipeline.create_file_name(self.name, sub_directory=self.name, file_suffix="controlRegions_distrib.txt")
                
            self._files.append(cn_rdata_file)
            self._files.append(cutoffs_file)
            self._files.append(blorente_plot)
            self._files.append(blorente_ctrl_regions_dstrb)
           
        return self._files
        
    def run(self):
        gem.copyNumberDistribution(
            self._input(True),
            output = self.files(),
            sampleName = self.pipeline.name
        )
        
class DuplicationStep(PipelineStep):
    """ Duplication call step """
    
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            
            duplication_file = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M1", 
                                                              file_suffix="final.calls.dups.bed")
            duplication_without_gaps_file = self.pipeline.create_file_name(self.name, sub_directory=self.name  + "/M1" , 
                                                                           file_suffix="final.calls.dups.woGaps.bed")
            
            filteredSwCall = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M2", file_suffix="calls.sw_norm_wochrXMY.bed")
            filteredLwCall = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M2", file_suffix="calls.lw_norm_wochrXMY.bed")
            filteredCwCall = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M2", file_suffix="calls.cw_norm_wochrXMY.bed")
            
            wssdPicked = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M2/WSSD", file_suffix="sd_woChrXMY.tab")
            wssdMerged = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M2/WSSD", file_suffix="sd_woChrXMY.merged")
            wssdMerged10K = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M2/WSSD", file_suffix="sd_woChrXMY_10K.merged")

            wssdMerged10KNoGaps = self.pipeline.create_file_name(self.name, sub_directory=self.name + "/M2/WSSD", file_suffix="sd_woChrXMY_10K.woGaps.merged")

            self._files.append(duplication_file)
            self._files.append(duplication_without_gaps_file)
            self._files.append(filteredSwCall)
            self._files.append(filteredLwCall)
            self._files.append(filteredCwCall)
            self._files.append(wssdPicked)
            self._files.append(wssdMerged)
            self._files.append(wssdMerged10K)
            self._files.append(wssdMerged10KNoGaps)
            
        return self._files
                
    def run(self):
        cfg = self.configuration
        gem.callDuplications(
            self._input(),
            output = self.files(),
            sampleName = self.pipeline.name,
            bed_repeat_regions = cfg["bed_repeat_regions"],
            bed_gaps_coordinates = cfg["bed_gaps_coordinates"]
        )
        
    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on mrCaNaVar and copy number analysis!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0]
        
class DocumentationBamStep(PipelineStep):
    """Documentation Bam Step"""
    
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            
            stats_file = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="txt")
            html_file = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="html")
            json_file = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="json")    
            
            self._files.append(stats_file)
            self._files.append(html_file)
            self._files.append(json_file)
            
        return self._files

    def run(self):
        cfg = self.configuration
        bam_file = self._input()[0][0]
        gem.runBamSummaryMetrics(input=bam_file,output=self.files()[0],picard_tools_path=cfg["picard_path"],java_heap=cfg["java_heap"],tmp_folder=cfg["tmp_folder"],reference=cfg["bwa_reference"])
        gem.cnvReport.create_bam_report(metrics=self.files()[0],html_file=self.files()[1],json_file=self.files()[2],sample_description=cfg["sample_description"])

    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on mapping BAM mapping generation step!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0]
        
class DocumentationRmDupStep(PipelineStep):
    """Build HTML and json documentation for the remove duplicates process"""
    
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            
            html_file = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="html")
            json_file = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="json")    
            
            self._files.append(html_file)
            self._files.append(json_file)
            
        return self._files
        
    def run(self):
        cfg = self.configuration
        metrics = self._input()[0][0]
        gem.cnvReport.create_duplicates_report(metrics=metrics,html_file=self.files()[0],json_file=self.files()[1],sample_description=cfg["sample_description"])

    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on mapping BAM mapping generation step!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0]
    

class DocumentationStep(PipelineStep):
    """Documentation Step"""
    
    def files(self):
        """Return the output files generated by this step. """
        if self._files is None:
            self._files = []
            
            html_file = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="html")
            json_file = self.pipeline.create_file_name(self.name, sub_directory=self.name,file_suffix="json")    
            
            self._files.append(html_file)
            self._files.append(json_file)
            
        return self._files
            
    def run(self):
        cfg = self.configuration

        filesOut = self.files()
        html_doc = filesOut[0]        
        json_doc = filesOut[1]
        
        inputsList = self._input()

        map_json_files = cfg["map_json_list"]
        
        calls_log_file = gem._prepare_calls_log(inputsList[0][0])
        
        dup_m1 = None
        dup_m2 = None
        if self.pipeline.duplications_create == True:        
            dup_m1 = inputsList[2][1]
            dup_m2 = inputsList[2][8]
        
        gem.cnvReport.create_report(html_file=html_doc,json_file=json_doc,mapping_stats_files=map_json_files, 
                                    mrcanavar_log_file=calls_log_file,control_regions_distribution=inputsList[1][3], 
                                    control_regions_plot=inputsList[1][2],cutoffs_file=inputsList[1][1], duplications_M1=dup_m1, 
                                    duplications_M2=dup_m2,sample_description=cfg["sample_description"])

    def _input(self):
        """Return the output of all
        dependencies"""
        if not self.dependencies:
            raise PipelineError("This step depends on mapping stats,mrCaNaVar,copy number analysis and duplications!")
        return [self.pipeline.steps[i].files() for i in self.dependencies if i >= 0]

class BasePipeline:
    """Basic Base Pipeline """
    
    def __init__(self, args=None):
        self.steps = []  # pipeline steps
        self.run_steps = []  # steps to run

        # general parameter
        self.input = None  # input files
        self.name = None  # target name
        self.names = []  # target names in case more than one set of fastq to be processes
        self.multiFastq = False #If more than a fastq file or a Pair End combination file is going to be processed
        self.sample_name_multi_fastq = None 
        
        self.remove_temp = True  # remove temporary
         
        self.output_dir = None  # Output directory
        self.threads = 1  # number of threads
        self.write_config = None  # write configuration
        self.dry = False  # only dry run
        self.sort_memory = "768M"  # samtools sort memory
        self.direct_input = False  # if true, skip the preparation step
        self.force = False  # force computation of all steps
       
        self.compress_all = False 
        self.compress = False
        
        #PicardTools
        self.picard_path = None
        self.java_heap = "25g"
        self.tmp_folder = "/tmp/"
        
        
        #Documentation files
        self.sample_description = None
        
        self.membersInitiation()
        
        if args is not None:
            # initialize from arguments
            # load configuration
            try:
                if args.load_configuration is not None:
                    self.load(args.load_configuration)
            except AttributeError:
                pass
            ## update parameter
            self.update(vars(args))
            ## initialize pipeline and check values
            self.initialize()
     
    def membersInitiation(self):
        #To fullfill in the child class
        pass
        
    def update(self, configuration):
        """Update configuration from given map

        configuration -- the input configuration
        """
        for k, v in configuration.items():
            try:
                if v is not None:
                    setattr(self, k, v)
            except AttributeError:
                pass

    def __update_dict(self, target, source):
        if source is None:
            return
        for k, v in source.items():
            #if v is not None:
            target[k] = v
            
    def open_input(self,pair_end_files = None):
        
        if pair_end_files is not None:
            return gem.filter.interleave([gem.files.open(f) for f in pair_end_files], threads=max(1, self.threads / 2))
        """Open the original input files"""
        if len(self.input) == 1:
            return gem.files.open(self.input[0])
        else:
            return gem.filter.interleave([gem.files.open(f) for f in self.input], threads=max(1, self.threads / 2))

    def open_step(self, id, raw=False):
        """Open the original input files"""
        return self.steps[id].open(raw=raw)
        
    def initialize(self, silent=False):
        # check general parameter
        errors = []
       
        if self.input is None:
            errors.append("No input file specified")
        else:
            if len(self.input) == 1 and not self.single_end:
                # search for second file
                (n, p) = gem.utils.find_pair(self.input[0])
                if p is None:
                    #errors.append("Unable to deduce second pair input file from %s " % self.input[0])
                    logging.gemtools.warning("No second input file specified, assuming interleaved paird end reads!")
                else:
                    logging.gemtools.warning("Second pair input file found: %s " % p)
                    if self.name is None:
                        self.name = n
                    self.input.append(p)

            # check file counts
            if self.single_end and len(self.input) != 1 and self.sample_name_multi_fastq is None:
                errors.append("Specify exactly one input file in single end mode")
            elif not self.single_end and self.sample_name_multi_fastq is not None:
                #PE and multiple fastq files for a given sample
                # check input files
                input_abs = []
                name_abs = []
                for f in self.input:
                    #f is not a name file that means is a list and comes from a configuration file
                    if type(f) is ListType:
                        f = f[0]
                    #search for second pair
                    (n, p) = gem.utils.find_pair(f)
                    if p is None:
                        errors.append("No second pair file found!!")
                    else:
                        logging.gemtools.warning("Second pair input file found: %s " % p)
                        input_abs.append([f,p])
                        name_abs.append(n)
                
                self.input = input_abs
                self.names = name_abs
                self.multiFastq = True  
                if self.sample_name_multi_fastq is None:
                    errors.append('''No sample name was defined. You must specify a sample name 
                                     when working with more than one combination of pair end files. You
                                     must use -sample-name-multi-fastq parameter''')    
                else:
                    self.name = self.sample_name_multi_fastq
                        
            else:
                # check input files
                input_abs = []
                for f in self.input:
                    if f is None or not os.path.exists(f):
                        errors.append("Input file not found: %s" % (f))
                    else:
                        # make aboslute path
                        input_abs.append(os.path.abspath(f))
                self.input = input_abs

        if self.name is None and self.input is not None and len(self.input) > 0:
            # get name from input files
            name = os.path.basename(self.input[0])
            if name.endswith(".gz"):
                name = name[:-3]
            idx = name.rfind(".")
            if idx > 0:
                self.name = name[:idx]

        if self.name is None or len(self.name) == 0:
            errors.append("No name specified and unable to guess one. Please use --name to set a name explicitly.")

        self.referenceCheck(errors)

        if self.output_dir is None:
            self.output_dir = os.getcwd()

        self.output_dir = os.path.abspath(self.output_dir)

        if self.threads <= 0:
            self.threads = 1

        self.noStandardParameterChecking(errors)
                
        if not silent and len(errors) > 0 and self.write_config is None:
            raise PipelineError("Failed to initialize neccessary parameters:\n\n%s" % ("\n".join(errors)))
        if self.write_config is not None:
            # log configuration errors
            logging.gemtools.warning("---------------------------------------------")
            logging.gemtools.warning("Writing configuration")
            logging.gemtools.warning("")
            logging.gemtools.warning("Note that some of the parameters are missing:\n")
            for e in errors:
                logging.gemtools.warning("\t" + str(e))
            logging.gemtools.warning("---------------------------------------------")
      
    def pairingInfoControl(self,fileDescriptor):
        '''Returns true if en error of pairing information must be reported'''
        # check pairing information
        p1 = None
        p2 = None
        c = 0
        inp = fileDescriptor
        for template in inp:
            if template.num_alignments == 2:
                ## paired alignment
                p1 = 1
                p2 = 2
                inp.close()
                break
            else:
                if c == 0:
                    p1 = template.get_pair()
                elif c == 1:
                    p2 = template.get_pair()
                c += 1
                if c >= 2:
                    inp.close()
                    break
        inp.close()
        if p1 == 0 or p2 == 0 or (p1 == 1 and p2 != 2) or (p2 == 1 and p1 != 2):
            return True
        else:
            return False
                      
    def referenceCheck(self,errors):
        """Reference checking, by default Checks gem index
        Parameters:
        errors - list of error messages
        This method must be redifines in Child Classes """
        pass
                
    def noStandardParameterChecking(self,errors):
        """No Standard Parameters checking
        Parameters:
        errors - list of error messages
        This method must be redifines in Child Classes """
        pass

    def log_parameter(self):
        """Print selected parameters"""
        printer = logging.gemtools.gt
        run_step = len(self.run_steps) > 0
        
        printer("------------ Input Parameter ------------")
        printer("Input File(s)    : %s", self.input)
        
        self.print_parameters(printer)
  
        printer("")

        if not run_step:
            printer("------------ Pipeline Steps  ------------")
            for i, s in enumerate(self.steps):
                printer("%-2d - %20s : %s", i, s.name, s.description)
        else:
            printer("------------ Single Step execution  ------------")

        for i, s in enumerate(self.steps):
            if run_step and s.id not in self.run_steps:
                continue
            printer("")
            if len(s.dependencies) > 0:
                printer("------------ [ID:{0:-3} -- '{1}'] [Depends On: {2}] ------------".format(i, s.name, ", ".join([self.steps[j].name for j in s.dependencies])))
            else:
                printer("------------ [ID:{0:-3} -- '{1}'] ------------".format(i, s.name))

            for k, v in s.configuration.items():
                printer("%25s : %s", k, str(v))

            for i, f in enumerate(s.files()):
                if i == 0:
                    printer("%25s : %s", "Temporary Outputs", not s.final)
                    printer("%25s : %s", "Outputs", f)
                else:
                    printer("%25s : %s", "", f)
        printer("--------------------------------------------------------------")
        printer("")  

    def print_parameters(self,printer):
        """Print Information Parameter of the class"""
        pass          
       
    def load(self, file):
        """Load pipeline configuration from file"""
        if file is None or not os.path.exists(file):
            raise PipelineError("Configuration file not found: %s" % file)
        fd = open(file, "r")
        logging.gemtools.info("Loading configuraiton from %s", file)
        data = json.load(fd)
        for k, v in data.items():
            if hasattr(self, k):
                setattr(self, k, v)
        fd.close()
        
    def write_pipeline(self, file_name):
        """Write the pipeline and its configuration to a file
        based on the name
        """

        json_container = dict(self.__dict__)
        # skip the steps here, we convert them manually
        del json_container["steps"]
        del json_container["run_steps"]
        del json_container["write_config"]
        # remove non default values
        default_pipeline = BasePipeline()
        default_pipeline.initialize(silent=True)
        for k, v in json_container.items():
            if hasattr(default_pipeline, k) and getattr(default_pipeline, k) == v:
                del json_container[k]

        # json_container['piepline_steps'] = json_steps

        fd = open(file_name, "w")
        json.dump(json_container, fd, indent=2, sort_keys=True)
        fd.close()
        logging.gemtools.gt("Configuration saved to %s\n", file_name)
        
    def run(self):
        run_step = len(self.run_steps) > 0

        if self.write_config is not None:
            self.write_pipeline(self.write_config)
            return
        if self.dry:
            # check and print states
            print "Checking Job states"
            print "-----------------------------------------------------"
            for step in self.steps:
                print "Step %3s:%25s :: %s" % (str(step.id), step.name,
                                         "Done" if step.is_done() else "Compute")
            return

        error = False

        all_done = True
        final_files = []
        # check final steps if we are not running a set of steps
        if not run_step and not self.force:
            for step in self.steps:
                if step.final:
                    final_files.extend(step.files())
                    all_done = all_done & step.is_done()
            if all_done:
                logging.gemtools.warning("The following files already exist. Nothing to be run!\n\n%s\n" % ("\n".join(final_files)))
                return

        time = Timer()
        times = {}

        if run_step:
            # sort by id
            self.run_steps.sort()
        ids = [s.id for s in self.steps]
        if run_step:
            ids = self.run_steps

        step = None

        # register signal handler to catch
        # interruptions and perform cleanup
         # register cleanup signal handler
        def cleanup_in_signal(signal, frame):
            logging.gemtools.warning("Job step canceled, forcing cleanup!")
            step.cleanup(force=True)

        signal.signal(signal.SIGINT, cleanup_in_signal)
        signal.signal(signal.SIGQUIT, cleanup_in_signal)
        signal.signal(signal.SIGHUP, cleanup_in_signal)
        signal.signal(signal.SIGTERM, cleanup_in_signal)

        for step_id in ids:
            step = self.steps[step_id]

            if run_step:
                # check dependencies are done
                for d in step.dependencies:
                    if not self.steps[d].is_done():
                        logging.gemtools.error("Step dependency is not completed : %s", self.steps[d].name)
                        error = True
                        break
            if run_step or self.force or not step.is_done():
                logging.gemtools.gt("Running step: %s" % step.name)
                t = Timer()

                if not os.path.exists(self.output_dir):
                    # make sure we create the ouput folder
                    logging.gemtools.warn("Creating output folder %s", self.output_dir)
                    try:
                        os.makedirs(self.output_dir)
                    except OSError as exc: # Python >2.5
                        if exc.errno == errno.EEXIST and os.path.isdir(path):
                            pass
                        else:
                            logging.gemtools.error("unable to create output folder %s", self.output_dir)
                            error = True
                            break

                try:
                    step.run()
                except KeyboardInterrupt:
                    logging.gemtools.warning("Job step canceled, forcing cleanup!")
                    error = True
                    step.cleanup(force=True)
                    break
                except PipelineError, e:
                    logging.gemtools.error("Error while executing step %s : %s" % (step.name, str(e)))
                    logging.gemtools.warning("Cleaning up after failed step : %s", step.name)
                    step.cleanup(force=True)
                    error = True
                    break
                except gem.utils.ProcessError, e:
                    logging.gemtools.error("Error while executing step %s : %s" % (step.name, str(e)))
                    logging.gemtools.warning("Cleaning up after failed step : %s", step.name)
                    step.cleanup(force=True)
                    error = True
                    break
                except Exception, e:
                    traceback.print_exc()
                    logging.gemtools.error("Error while executing step %s : %s" % (step.name, str(e)))
                    logging.gemtools.warning("Cleaning up after failed step : %s", step.name)
                    step.cleanup(force=True)
                    error = True
                    break
                finally:
                    t.stop(step.name + " completed in %s", loglevel=None)
                    times[step.id] = t.end
                    if not error:
                        logging.gemtools.gt("Step %s finished in : %s", step.name, t.end)
                    else:
                        logging.gemtools.gt("Step %s failed after : %s", step.name, t.end)
            else:
                logging.gemtools.warning("Skipping step %s, output already exists" % (step.name))

        # do celanup if not in error state
        if not error:
            logging.gemtools.debug("Cleanup after run")
            for step in self.steps:
                step.cleanup()

            time.stop("Completed in %s", loglevel=None)
            logging.gemtools.gt("Step Times")
            logging.gemtools.gt("-------------------------------------")
            for s in self.steps:
                if s.id in times:
                    logging.gemtools.gt("{0:>25} : {1}".format(s.name, times[s.id]))
                else:
                    logging.gemtools.gt("{0:>25} : skipped".format(s.name))
            logging.gemtools.gt("-------------------------------------")
            logging.gemtools.gt("Pipeline run finshed in %s", time.end)

    def create_file_name(self,suffix,sub_directory=None, name_suffix=None, file_suffix="map", final=False, multi_fastq = False):
        """Create a result file name"""
        file = ""
        files = []
        
        if final:
            suffix = None
        if name_suffix is None:
            name_suffix = ""
            
        out_path = self.output_dir
        if sub_directory is not None:
            out_path = out_path + "/" + sub_directory
            
        #Checking output path    
        if not os.path.exists(out_path):
            try:
                os.makedirs(out_path)
            except OSError, e:
                logging.gemtools.gt("It was not possible to create directory  %s", out_path)
                

        if multi_fastq == True:
            for name in self.names:
                if suffix is not None and len(suffix) > 0:
                    file = "%s/%s%s_%s.%s" % (out_path, name, name_suffix, suffix, file_suffix)
                else:
                    file = "%s/%s%s.%s" % (out_path, name, name_suffix, file_suffix)
                if (self.compress_all or final and self.compress) and file_suffix in ["map", "fastq"]:
                    file += ".gz"
                files.append(file)
        
            return files   
        else:            
            if suffix is not None and len(suffix) > 0:
                file = "%s/%s%s_%s.%s" % (out_path, self.name, name_suffix, suffix, file_suffix)
            else:
                file = "%s/%s%s.%s" % (out_path, self.name, name_suffix, file_suffix)
            if (self.compress_all or final and self.compress) and file_suffix in ["map", "fastq"]:
                file += ".gz"
        
            return file
            
    def register_parameter(self, parser):
        """Register all parameters with the given
        arparse parser"""
        self.register_general(parser)
        self.register_class_parameters(parser)
        self.register_output(parser)
        self.register_execution(parser)
        
    def register_general(self, parser):
        """Register all general parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        input_group = parser.add_argument_group('Input')
        ## general pipeline paramters
        input_group.add_argument('-f', '--files', dest="input", nargs="+", metavar="input",
            help='''Single fastq input file or both files for a paired-end run separated by space.
            If your sample have a set of fastq PE files, specify just the pair one files, we will 
            look automatically for the pair two file.
            Note that if you specify only one file, we will look for the file containing the other pairs
            automatically and start a paired-end run. Add the --single-end parameter to disable
            pairing and file search. 
            The file search for the second pair detects pairs ending in [_|.|-][0|1|2].[fq|fastq|txt][.gz].''')
        input_group.add_argument('--single-end', dest="single_end", action="store_true", default=None, help="Single end reads")
        input_group.add_argument('--sample-name-multi-fastq',dest="sample_name_multi_fastq", metavar="SAMPLE_NAME",
                                 help='''Sample name used for the name of results when running diferent fastq pair end files ''')
        input_group.add_argument('-q', '--quality', dest="quality", metavar="quality",
            default=self.quality, help='Quality offset. 33, 64 or "ignore" to disable qualities.')
        input_group.add_argument('-i', '--index', dest="index", metavar="index", help='Path to the .gem genome index')
        
    def register_class_parameters(self,parser):
        """Register class parameters, to be defined in the child class"""
        pass
    
    def register_output(self, parser):
        """Register all output parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        output_group = parser.add_argument_group('Output')
        output_group.add_argument('-n', '--name', dest="name", metavar="name", help="""Name used for the results. If not specified, the name is inferred from
            the input files""")
        output_group.add_argument('-o', '--output-dir', dest="output_dir", metavar="dir", help='Optional output folder. If not specified the current working directory is used.')
        output_group.add_argument('-g', '--no-gzip', dest="compress", action="store_false", default=None, help="Do not compress final mapping file")
        output_group.add_argument('--compress-all', dest="compress_all", action="store_true", default=None, help="Compress all intermediate output")
        output_group.add_argument('--keep-temp', dest="remove_temp", action="store_false", default=None, help="Keep temporary files")
        
    def register_execution(self, parser):
        """Register the execution mapping parameters with the
        given arparse parser

        parser -- the argparse parser
        """
        execution_group = parser.add_argument_group('Execution')
        execution_group.add_argument('--save', dest="write_config", nargs="?", const=None, help="Write the given configuration to disk")
        execution_group.add_argument('--dry', dest="dry", action="store_true", default=None, help="Print and write configuration but do not start the pipeline")
        execution_group.add_argument('--load', dest="load_configuration", default=None, metavar="cfg", help="Load pipeline configuration from file")
        execution_group.add_argument('--run', dest="run_steps", type=int, default=None, nargs="+", metavar="cfg", help="Run given pipeline steps idenfified by the step id")
        execution_group.add_argument('--force', dest="force", default=None, action="store_true", help="Force running all steps and skip checking for completed steps")
       

class BwaMemPipeline(BasePipeline):
    """ Basic Mapping with BWA MEM """ 
    def membersInitiation(self):
        #Basic Mapping Pipeline Values
        self.bwa_reference = None #Path to the bwa reference  
        self.java_heap = "5g"
        self.quality = None  # quality offset
        self.picard_path = False #Picard Path
        self.single_end = False 
        
    def referenceCheck(self,errors):
        """Reference checking, by default Checks gem index
        Parameters:
        errors - list of error messages"""
        
        if self.bwa_reference is None:
            errors.append("No reference specified")
        else:
            if not os.path.exists(self.bwa_reference):
                errors.append("Reference not found: %s" % (self.bwa_reference))
            else:
                self.bwa_reference = os.path.abspath(self.bwa_reference)
    
    def noStandardParameterChecking(self,errors):
        """No Standard Parameters checking
        Parameters:
        errors - list of error messages
        This method must be redifines in Child Classes """
        pass
        
    def bwaMemMapping(self, name, description="",dependencies=None,configuration=None, final=False):
        """Add base mapping step"""
        step = BwaMappingStep(name, final=final, dependencies=dependencies, description=description, file_suffix="bam")
        config = dotdict()
        
        config.bwa_reference = self.bwa_reference
        config.threads = self.threads
        config.tmp_folder = self.tmp_folder
        
        if configuration is not None:
            self.__update_dict(config, configuration)
            
        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def htmlDocumentation(self,name, suffix=None, configuration=None, 
                    dependencies=None, final=False, description="Create html and Json Report"):
        """ Creates HTML and JSON Documetation  """
        step = DocumentationBamStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()
        
        config.sample_description = self.sample_description
        config.picard_path = self.picard_path
        config.java_heap = self.java_heap
        config.tmp_folder = self.tmp_folder
        config.bwa_reference = self.bwa_reference
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def print_parameters(self,printer):
        """Print class parameters"""
        printer("Bwa Reference    : %s", self.bwa_reference)
        printer("Threads_number   : %s", self.threads) 
        printer("")
    
    
    def register_general(self, parser):
        """Register all general parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        input_group = parser.add_argument_group('Input')
        ## general pipeline paramters
        input_group.add_argument('-f', '--files', dest="input", nargs="+", metavar="input",
            help='''Single fastq input file or both files for a paired-end run separated by space.
            If your sample have a set of fastq PE files, specify just the pair one files, we will 
            look automatically for the pair two file.
            Note that if you specify only one file, we will look for the file containing the other pairs
            automatically and start a paired-end run. Add the --single-end parameter to disable
            pairing and file search. 
            The file search for the second pair detects pairs ending in [_|.|-][0|1|2].[fq|fastq|txt][.gz].''')
        input_group.add_argument('--single-end', dest="single_end", action="store_true", default=None, help="Single end reads")
       
    def register_class_parameters(self,parser):
        """Class Parametere Registration"""
        self.register_baseMapping(parser)
        self.register_htmlDocumentation(parser)
        
    def register_baseMapping(self,parser):
        """Register all base mapping parameters with given
        argparse parser

        parser -- the argparse parser
        """
        baseMapping_group = parser.add_argument_group('BWA mem Mapping for removing PCR Duplicates')
        baseMapping_group.add_argument('-bwa-ref', dest="bwa_reference", metavar="bwa_reference", 
                                        help='''Path to the fasta genome file reference. BWA index file
                                        must be at the same folder ''')  
        #-T threads_number
        baseMapping_group.add_argument('-T','--threads',type=int,dest="threads", metavar="t", 
                                   help='Number of threads. Default to %d' % self.threads)
                        
    def register_htmlDocumentation(self,parser):
        """Register the documentation parameters with the
        given arparse parser

        parser -- the argparse parser
        """
        documentation_group = parser.add_argument_group('Documentation')        
        documentation_group.add_argument('-sample-description',dest="sample_description",metavar="SAMPLE_DESCRIPTION",
                                         help='''Description text for the sample''')
        picard_group = parser.add_argument_group('Bam Statistics using CollectAlignmentSummaryMetrics from PicardTools')
        picard_group.add_argument('-picard-path', dest="picard_path", metavar="PICARD_PATH", 
                                        help='''Path to the PicardTools folder installation.
                                        The directory were are located .jar Picard applications''')    
        picard_group.add_argument('-java-heap',dest="java_heap", 
                                         default=self.java_heap,
                                         metavar="Xg",
                                         help='''Memory to reserve for the Java Heap Memory Stack
                                         PicardTools is a Java application than runs over a virtual machine
                                         that must nead a Jave Heap size defined. By default 5g''')
                                         
        picard_group.add_argument('-tmp-folder',dest="tmp_folder", 
                                         default=self.tmp_folder,
                                         metavar="TMP_PATH",
                                         help='''Temporary directory or envioroment variable to store
                                         temporary files. Examples: /tmp or $TMPDIR''')
                                        
    def register_output(self, parser):
        """Register all output parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        output_group = parser.add_argument_group('Output')
        output_group.add_argument('-n', '--name', dest="name", metavar="name", help="""Name used for the results. If not specified, the name is inferred from
            the input files""")
        output_group.add_argument('-o', '--output-dir', dest="output_dir", metavar="dir", help='Optional output folder. If not specified the current working directory is used.')


class RmDupPipeline(BasePipeline):
    """ Remove Duplicates Pipeline """  
    def membersInitiation(self):
        #Basic Mapping Pipeline Values
        self.java_heap = "25g"
        self.picard_path = False #Picard Path  
        self.tmp_folder = "/tmp/"
        self.merge_bams = False
        
    def referenceCheck(self,errors):
        """Reference checking, by default Checks gem index
        Parameters:
        errors - list of error messages"""
        pass
    
    def noStandardParameterChecking(self,errors):
        """No Standard Parameters checking
        Parameters:
        errors - list of error messages
        """
        if len(self.input) > 1:
            self.merge_bams = True
    
    def mergeMappings(self, name, description="",dependencies=None,configuration=None, final=False):
        """Merge Mapping Step"""
        step = MergeMappingStep(name, final=final, dependencies=dependencies, description=description, file_suffix="bam")
        config = dotdict()
        
        config.threads = self.threads        
        
        if configuration is not None:
            self.__update_dict(config, configuration)
            
        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def removeDuplicates(self, name, description="",dependencies=None,configuration=None, final=False):
        """Remove PCR-Duplicates"""
        step = MarkDuplicatesStep(name, dependencies=dependencies, final=final, description=description, file_suffix="rmdup")
        config = dotdict()
        
        config.picard_path = self.picard_path 
        config.java_heap = self.java_heap
        config.tmp_folder = self.tmp_folder
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id        
        
    def htmlDocumentation(self,name, suffix=None, configuration=None, 
                    dependencies=None, final=False, description="Create html and Json Report"):
        """ Creates HTML and JSON Documetation  """
        step = DocumentationRmDupStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()
        
        config.sample_description = self.sample_description
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id  
        
    def initialize(self, silent=False):
        # check general parameter
        errors = []
       
        if self.input is None:
            errors.append("No input file specified")
        else:
            # check input files
            input_abs = []
            for f in self.input:
                if f is None or not os.path.exists(f):
                    errors.append("Input file not found: %s" % (f))
                else:
                    # make aboslute path
                    input_abs.append(os.path.abspath(f))
            self.input = input_abs

        if self.name is None and self.sample_description is not None:
            self.name = self.sample_description

        if self.name is None and self.input is not None and len(self.input) > 0:
            # get name from input files
            name = os.path.basename(self.input[0])
            if name.endswith(".bam"):
                name = name[:-4]
            idx = name.rfind(".")
            if idx > 0:
                self.name = name[:idx]

        if self.name is None or len(self.name) == 0:
            errors.append("No name specified and unable to guess one. Please use --name to set a name explicitly.")

        self.referenceCheck(errors)

        if self.output_dir is None:
            self.output_dir = os.getcwd()

        self.output_dir = os.path.abspath(self.output_dir)

        self.noStandardParameterChecking(errors)

        if not silent and len(errors) > 0 and self.write_config is None:
            raise PipelineError("Failed to initialize neccessary parameters:\n\n%s" % ("\n".join(errors)))
        if self.write_config is not None:
            # log configuration errors
            logging.gemtools.warning("---------------------------------------------")
            logging.gemtools.warning("Writing configuration")
            logging.gemtools.warning("")
            logging.gemtools.warning("Note that some of the parameters are missing:\n")
            for e in errors:
                logging.gemtools.warning("\t" + str(e))
            logging.gemtools.warning("---------------------------------------------")
            
    def print_parameters(self,printer):
        """Print class parameters"""
        printer("Picard Path    : %s", self.picard_path)
        printer("Java Heap      : %s", self.java_heap) 
        printer("TMP folder     : %s", self.tmp_folder) 
        printer("Threads_number   : %s", self.threads)
        printer("")
        
    def register_general(self, parser):
        """Register all general parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        input_group = parser.add_argument_group('Input')
        ## general pipeline paramters
        input_group.add_argument('-f', '--files', dest="input", nargs="+", metavar="input",
            help='''One or more bam files. For more than one bam file a bam merging will be performed
            before running PCR remove duplicates''')
            
        #-T threads_number
        input_group.add_argument('-T','--threads',type=int,dest="threads", metavar="t", 
                                   help='Number of threads. Default to %d' % self.threads)
            
                    
    def register_class_parameters(self,parser):
        """Class Parametere Registration"""
        self.register_removeDuplicates(parser)
        self.register_htmlDocumentation(parser)
        
    def register_removeDuplicates(self, parser):   
        """Register all PCR Duplicates parameters with given
        argparse parser

        parser -- the argparse parser
        """
        pcrDuplicates_group = parser.add_argument_group('PCR Duplicates removing process using MarkDuplicates from PicardTools')
        pcrDuplicates_group.add_argument('-picard-path', dest="picard_path", metavar="PICARD_PATH", 
                                        help='''Path to the PicardTools folder installation.
                                        The directory were are located .jar Picard applications''')    
        pcrDuplicates_group.add_argument('-java-heap',dest="java_heap", 
                                         default=self.java_heap,
                                         metavar="Xg",
                                         help='''Memory to reserve for the Java Heap Memory Stack
                                         PicardTools is a Java application than runs over a virtual machine
                                         that must nead a Jave Heap size defined. By default 25g''')
                                         
        pcrDuplicates_group.add_argument('-tmp-folder',dest="tmp_folder", 
                                         default=self.tmp_folder,
                                         metavar="TMP_PATH",
                                         help='''Temporary directory or envioroment variable to store
                                         temporary files. Examples: /tmp or $TMPDIR''') 

    def register_htmlDocumentation(self,parser):
        """Register the documentation parameters with the
        given arparse parser

        parser -- the argparse parser
        """
        documentation_group = parser.add_argument_group('Documentation')        
        documentation_group.add_argument('-sample-description',dest="sample_description",metavar="SAMPLE_DESCRIPTION",
                                         help='''Description text for the sample''')
                                         
    
    def register_output(self, parser):
        """Register all output parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        output_group = parser.add_argument_group('Output')
        output_group.add_argument('-n', '--name', dest="name", metavar="name", help="""Name used for the results. If not specified, the name is inferred from
            the input files""")
        output_group.add_argument('-o', '--output-dir', dest="output_dir", metavar="dir", help='Optional output folder. If not specified the current working directory is used.')
   
       
class Bam2FastqPipeline(BasePipeline):
    """Bam to fastq Pipeline class."""
   
    def membersInitiation(self):
        #Basic Mapping Pipeline Values
        self.split_times = 100
        self.kmer_length = 36
        self.windowing = 36
        self.first_position = 10
        self.single_end = False  # single end alignments
        self.threads = 1
       
    def referenceCheck(self,errors):
        """Reference checking, by default Checks gem index
        Parameters:
        errors - list of error messages"""
        pass
    
    def noStandardParameterChecking(self,errors):
        """No Standard Parameters checking
        Parameters:
        errors - list of error messages
        """
        pass
    
    def bamToFastq(self, name, description="",dependencies=None,configuration=None, final=False):
        """ Transforms bam file to fastq free or PCR Duplicates """
        step = BamToFastq(name, dependencies=dependencies, final=final, description=description, file_suffix="fastq")
        config = dotdict()
        
        if configuration is not None:
           self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def fragmentReads(self,name,dependencies=None,configuration=None,final=False,description="Break reads"):
        """ Fragment reads """
        step = FragmentReadsStep(name, dependencies=dependencies, final=final, description=description, file_suffix="fq")
        config = dotdict()
        
        #config.fragmentation_reads = self.fragmentation_reads
        config.split_times = self.split_times
        config.gzipped = False
        config.threads = self.threads
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id        
        
        
    def chopReads(self, name, configuration=None, dependencies=None, final=True, description="Chop reads"):
        """ Reads chopped to 36 bp """
        step = ChopStep(name, dependencies=dependencies, final=final, description=description, file_suffix="fq")
        config = dotdict()
        
        config.split_times = self.split_times
        config.kmer_length = self.kmer_length
        config.windowing = self.windowing
        config.first_position = self.first_position
        config.threads = self.threads
           
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
 
    def initialize(self, silent=False):
        # check general parameter
        errors = []
       
        if self.input is None:
            errors.append("No input file specified")
        else:
            # check input files
            if self.input is None or not os.path.exists(self.input):
                errors.append("Input file not found: %s" % (self.input))
            else:
                # make aboslute path
                self.input = os.path.abspath(self.input)
            

        if self.name is None and self.input is not None:
            # get name from input files
            name = os.path.basename(self.input)
            if name.endswith(".bam"):
                name = name[:-4]
            idx = name.rfind(".")
            if idx > 0:
                self.name = name[:idx]

        if self.name is None or len(self.name) == 0:
            errors.append("No name specified and unable to guess one. Please use --name to set a name explicitly.")

        self.referenceCheck(errors)

        if self.output_dir is None:
            self.output_dir = os.getcwd()

        self.output_dir = os.path.abspath(self.output_dir)

        self.noStandardParameterChecking(errors)

        if not silent and len(errors) > 0 and self.write_config is None:
            raise PipelineError("Failed to initialize neccessary parameters:\n\n%s" % ("\n".join(errors)))
        if self.write_config is not None:
            # log configuration errors
            logging.gemtools.warning("---------------------------------------------")
            logging.gemtools.warning("Writing configuration")
            logging.gemtools.warning("")
            logging.gemtools.warning("Note that some of the parameters are missing:\n")
            for e in errors:
                logging.gemtools.warning("\t" + str(e))
            logging.gemtools.warning("---------------------------------------------")       

    
    def print_parameters(self,printer):
        """Print class parameters"""
        printer("Split times    : %i", self.split_times)
        printer("Kmer Length    : %i", self.kmer_length) 
        printer("Windowing      : %i", self.windowing)
        printer("First Position : %i", self.first_position)
        printer("Single End     : %s", self.single_end)
        printer("Threads        : %s", self.threads)
        printer("")
    
    def register_general(self, parser):
        """Register all general parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        input_group = parser.add_argument_group('Input')
        ## general pipeline paramters
        input_group.add_argument('-f', '--file', dest="input", metavar="input",
            help='''BAM mapping file free of PCR duplicates''')
        input_group.add_argument('--single-end', dest="single_end", action="store_true", default=None, help="Single end reads")
        input_group.add_argument('-T','--threads',type=int,dest="threads", metavar="t", 
                                   help='Number of threads. Default to %d' % self.threads)
            
    def register_class_parameters(self,parser):
        """Class Parameters Registration"""
        self.register_bam_to_fastq(parser)
        
    def register_bam_to_fastq(self,parser):
        """Register bam to fastq parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        bam_to_fastq_group = parser.add_argument_group('BAM to FASTQ and FASTQ fragmentation.')
        bam_to_fastq_group.add_argument('-split-times', dest="split_times", type=int, metavar="SPLIT_TIMES", 
                                        help='''Number of times to split the set of sequencing reads.
                                        The more splitted, the more jobs in parallel. Default "%i"''' % (self.split_times))
        bam_to_fastq_group.add_argument('-kmer-length', dest="kmer_length", type=int, metavar="KMER_LENGTH", 
                                        help='''Reads must be chopped to a given size. Default "%i"''' % (self.kmer_length))
        bam_to_fastq_group.add_argument('-windowing', type=int, dest="windowing", metavar="WINDOW_SIZE", 
                                        help='''While chopping reads you can specify a bp window sliding, if you are not interested
                                        in any overlap leave this value as the kmer-length. Default "%i"''' % (self.windowing))
        bam_to_fastq_group.add_argument('-first-position', type=int, dest="first_positions", metavar="POSITION_TO_START_CHOPPING", 
                                        help='''Position to start chopping your reads, if you are detecting bad patterns
                                        at the begining of the reads you can remove such bases. As a filtering criteria.  Default "%i"''' % (self.first_position))
                                        
                                        
    def register_output(self, parser):
        """Register all output parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        output_group = parser.add_argument_group('Output')
        output_group.add_argument('-n', '--name', dest="name", metavar="name", help="""Name used for the results. If not specified, the name is inferred from
            the input files""")
        output_group.add_argument('-o', '--output-dir', dest="output_dir", metavar="dir", help='Optional output folder. If not specified the current working directory is used.')
   
  
class SplitChopFastqPipeline(BasePipeline):  
    """Bam to fastq Pipeline class."""
   
    def membersInitiation(self):
        #Basic Mapping Pipeline Values
        self.split_times = 100
        self.kmer_length = 36
        self.windowing = 36
        self.first_position = 10
        self.single_end = False  # single end alignments
        self.gzipped = False #Input fastq file is gzipped
        self.threads = 1 #Number of threads
      
    def referenceCheck(self,errors):
        """Reference checking, by default Checks gem index
        Parameters:
        errors - list of error messages"""
        pass

    def noStandardParameterChecking(self,errors):
        """No Standard Parameters checking
        Parameters:
        errors - list of error messages
        """
        pass  
    
    def fragmentReads(self,name="",configuration=None, dependencies=None, final=False, description="Reads fragmentation"):
        """ Fragment reads from a FASTQ files in chunks """
        step = FragmentReadsStep(name, dependencies=dependencies, final=final, description=description, file_suffix="fq")
        config = dotdict()
        
        #config.fragmentation_reads = self.fragmentation_reads
        config.split_times = self.split_times
        config.gzipped = self.gzipped
        config.threads = self.threads
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
    
    def chopReads(self, name, configuration=None, dependencies=None, final=False, description="Break and chop reads"):
        """ Chop reads, reads chopped to 36 bp """
        step = ChopStep(name, dependencies=dependencies, final=final, description=description, file_suffix="fq")
        config = dotdict()
        
        config.split_times = self.split_times
        config.kmer_length = self.kmer_length
        config.windowing = self.windowing
        config.first_position = self.first_position
        config.threads = self.threads
           
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def print_parameters(self,printer):
        """Print class parameters"""
        printer("Split times    : %i", self.split_times)
        printer("Kmer Length    : %i", self.kmer_length) 
        printer("Windowing      : %i", self.windowing)
        printer("First Position : %i", self.first_position)
        printer("Single End     : %s", self.single_end)
        printer("FQ gzip        : %s", self.gzipped)
        printer("Threads        : %s", self.threads)

        printer("")
    
    def register_general(self, parser):
        """Register all general parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        input_group = parser.add_argument_group('Input')
        ## general pipeline paramters
        input_group.add_argument('-f', '--files', dest="input",nargs="+", metavar="input",
            help='''Single fastq input file or both files for a paired-end run separated by space.
            If you specify only one file, we will look for the file containing the other pairs
            automatically and start a paired-end run. Add the --single-end parameter to disable
            pairing and file search. 
            The file search for the second pair detects pairs ending in [_|.|-][0|1|2].[fq|fastq|txt][.gz].''')
        input_group.add_argument('--single-end', dest="single_end", action="store_true", default=None, help="Single end reads")
        input_group.add_argument('--gz', dest="gzipped", action="store_true", default=None, help="FASTQ files are gzipped")
        input_group.add_argument('-T','--threads',type=int,dest="threads", metavar="t", 
                                   help='Number of threads. Default to %d' % self.threads)
        
            
    def register_class_parameters(self,parser):
        """Class Parameters Registration"""
        self.register_split_chop_fastq(parser)
        
    def register_split_chop_fastq(self,parser):
        """Register split chop fastq parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        split_chop_fastq_group = parser.add_argument_group('SPLIT and FASTQ chopping.')
        split_chop_fastq_group.add_argument('-split-times', dest="split_times", type=int, metavar="SPLIT_TIMES", 
                                        help='''Number of times to split the set of sequencing reads.
                                        The more splitted, the more jobs in parallel. Default "%i"''' % (self.split_times))
        split_chop_fastq_group.add_argument('-kmer-length', dest="kmer_length", type=int, metavar="KMER_LENGTH", 
                                        help='''Reads must be chopped to a given size. Default "%i"''' % (self.kmer_length))
        split_chop_fastq_group.add_argument('-windowing', type=int, dest="windowing", metavar="WINDOW_SIZE", 
                                        help='''While chopping reads you can specify a bp window sliding, if you are not interested
                                        in any overlap leave this value as the kmer-length. Default "%i"''' % (self.windowing))
        split_chop_fastq_group.add_argument('-first-position', type=int, dest="first_positions", metavar="POSITION_TO_START_CHOPPING", 
                                        help='''Position to start chopping your reads, if you are detecting bad patterns
                                        at the begining of the reads you can remove such bases. As a filtering criteria.  Default "%i"''' % (self.first_position))
                                                                                
    def register_output(self, parser):
        """Register all output parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        output_group = parser.add_argument_group('Output')
        output_group.add_argument('-n', '--name', dest="name", metavar="name", help="""Name used for the results. If not specified, the name is inferred from
            the input files""")
        output_group.add_argument('-o', '--output-dir', dest="output_dir", metavar="dir", help='Optional output folder. If not specified the current working directory is used.')
   
        
  
class CopyNumberMappingPipeline(BasePipeline):
    """Copy Number Mapping Pipeline"""

    def membersInitiation(self):
        #Basic Mapping Pipeline Values
        self.genome_mismatch_alphabet = "ACGT" #mismatch-alphabet
        self.genome_mismatches = 2 #-m max_mismatches
        self.genome_max_edit_distance = 4 #-e max_edit_distance
        self.genome_strata_after_best = 2 #-s strata-after-best
        self.genome_max_decoded_matches = 20 #-d max-decoded-matches
        self.genome_min_decoded_strata = 1 #-D min-decoded-strata
        self.genome_threads = 8 #-T threads_number 
        self.quality = "ignore"  # quality offset
        
    def referenceCheck(self,errors):
        """Reference checking, by default Checks gem index
        Parameters:
        errors - list of error messages"""
        if self.index is None:
            errors.append("No index specified")
        else:
            if not os.path.exists(self.index):
                errors.append("Index not found: %s" % (self.index))
            else:
                self.index = os.path.abspath(self.index)
    
    def noStandardParameterChecking(self,errors):
        """No Standard Parameters checking
        Parameters:
        errors - list of error messages
        """
        # check inpuf compression
        if self.compress_all and not self.direct_input:
            logging.gemtools.warning("Enabeling direct input for compressed temporay files")
            self.direct_input = True

        if self.genome_min_decoded_strata >= self.genome_max_decoded_matches:
            errors.append("Invalid filtering configuration, min-strata >= max-strata!")
        else:
            if self.genome_mismatches <= 0:
                errors.append("Invalid filtering configuration, max-matches <= 0!")

    
    def cnvMapping(self, name, configuration=None, dependencies=None, final=False, description="Map chopped reads to a Masked Genome"):
        """ CNV Mapping using GEM """
        step = CnvMapping(name, dependencies=dependencies,final=final,description=description,file_suffix="map")
        config = dotdict()

        #Gem index reference
        config.index = self.index        
        #mismatch-alphabet
        config.mismatch_alphabet = self.genome_mismatch_alphabet
        #-m max_mismatches
        config.mismatches = self.genome_mismatches
        #-e max_edit_distance
        config.max_edit_distance = self.genome_max_edit_distance        
        #-s strata-after-best
        config.strata_after_best = self.genome_strata_after_best        
        #-d max-decoded-matches
        config.max_decoded_matches = self.genome_max_decoded_matches
        #-D min-decoded-strata
        config.min_decoded_strata = self.genome_min_decoded_strata
        #-T threads_number
        config.genome_threads = self.genome_threads

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
        
    def mappingStats(self, name, suffix=None, configuration=None, dependencies=None, final=False, description="Create mapping stats"):
        """ Mapping statistics """        
        step = MappingStatsStep(name, dependencies=dependencies, final=final, description=description, name_suffix=suffix)
        config = dotdict()

        config.mapping_stats_json = self.mapping_stats_json
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def mapToSam(self, name, suffix=None, configuration=None, dependencies=None, final=False, description="Translate from map to sam"):
        """ Translate map to sam file """
        step = MapToSamStep(name, dependencies=dependencies, final=final, description=description, file_suffix="map")
        config = dotdict()
        
        #Gem index reference
        config.index = self.index   
        config.genome_threads = self.genome_threads
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def initialize(self, silent=False):
        # check general parameter
        errors = []
       
        if self.input is None:
            errors.append("No input file specified")
        else:
            # check input files
            if self.input is None or not os.path.exists(self.input):
                errors.append("Input file not found: %s" % (self.input))
            else:
                # make aboslute path
                self.input = os.path.abspath(self.input)

        if self.name is None and self.input is not None:
            # get name from input files
            name = os.path.basename(self.input)
            if name.endswith(".gz"):
                name = name[:-3]
            idx = name.rfind(".")
            if idx > 0:
                self.name = name[:idx]

        if self.name is None or len(self.name) == 0:
            errors.append("No name specified and unable to guess one. Please use --name to set a name explicitly.")

        self.referenceCheck(errors)
        
        if self.quality is None:
            errors.append("You have to specify a quality offset (33, 64, or 'ignore' to disable)")
        elif str(self.quality) not in ["33", "64", "ignore", "offset-33", "offset-64"]:
            errors.append("Unknown quality offset: %s, please use 33, 64 or ignore" % (str(self.quality)))

        if self.output_dir is None:
            self.output_dir = os.getcwd()

        self.output_dir = os.path.abspath(self.output_dir)

        self.noStandardParameterChecking(errors)

        if not silent and len(errors) > 0 and self.write_config is None:
            raise PipelineError("Failed to initialize neccessary parameters:\n\n%s" % ("\n".join(errors)))
        if self.write_config is not None:
            # log configuration errors
            logging.gemtools.warning("---------------------------------------------")
            logging.gemtools.warning("Writing configuration")
            logging.gemtools.warning("")
            logging.gemtools.warning("Note that some of the parameters are missing:\n")
            for e in errors:
                logging.gemtools.warning("\t" + str(e))
            logging.gemtools.warning("---------------------------------------------")
            
    def print_parameters(self,printer):
        """Print class parameters"""
        printer("Mismatch Alphabet             : %s", self.genome_mismatch_alphabet)
        printer("Mismatches                    : %i", self.genome_mismatches) 
        printer("Max Edit Distance             : %i", self.genome_max_edit_distance)
        printer("Strata after best             : %i", self.genome_strata_after_best)
        printer("Max Decoded matches           : %s", self.genome_max_decoded_matches)
        printer("Min Decoded matches           : %s", self.genome_min_decoded_strata)
        printer("Threads                       : %s", self.genome_threads)
        printer("Quality                       : %s", self.quality)
        
        printer("")
        
    def register_general(self, parser):
        """Register all general parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        input_group = parser.add_argument_group('Input')
        ## general pipeline paramters
        input_group.add_argument('-f', '--file', dest="input", metavar="input",
            help='''FASTQ Single End file. It is expected to be a break fastq file.''')
        input_group.add_argument('-q', '--quality', dest="quality", metavar="quality",
            default=self.quality, help='Quality offset. 33, 64 or "ignore" to disable qualities.')
        input_group.add_argument('-i', '--index', dest="index", metavar="index", help='Path to the .gem genome index')
        
    def register_class_parameters(self,parser):
        """Class Parameters Registration"""
        self.register_cnvMapping(parser)
        self.register_mappingStats(parser)
        
    def register_cnvMapping(self,parser):
        """Register the genome cnv mapping parameters with the
        given arparse parser

        parser -- the argparse parser
        """
        # genome mapping parameter
        mapping_group = parser.add_argument_group('General mapping parameters')
        #mismatch-alphabet
        mapping_group.add_argument('--mismatch-alphabet', dest="genome_mismatch_alphabet", metavar="alphabet", 
                                   help='The mismatch alphabet. Default "%s"' % (self.genome_mismatch_alphabet))
        #-m max_mismatches
        mapping_group.add_argument('-m', '--mismatches', dest="genome_mismatches", type=int, metavar="mm", 
                                   help='Set the allowed mismatch ratio as 0 < mm < 1. Default %d' % (self.genome_mismatches))
        #-e max_edit_distance
        mapping_group.add_argument('-e','--max-edit-distance', dest="genome_max_edit_distance",  type=int, metavar="med", 
                                  help='Maximum edit distance (ratio) allowed for an alignment. Default %d' % (self.genome_max_edit_distance))
        #-s strata-after-best
        mapping_group.add_argument('-s', '--strata-after-best', dest="genome_strata_after_best", type=int, metavar="strata", 
                                   help='The number of strata examined after the best one. Default %d' % (self.genome_strata_after_best))
        #-d max-decoded-matches
        mapping_group.add_argument('-d','--max-decoded-matches', dest="genome_max_decoded_matches", metavar="mdm", 
                                   help='Maximum decoded matches. Default %d' % (self.genome_max_decoded_matches))
        #-D min-decoded-strata
        mapping_group.add_argument('-D','--min-decoded-strata', dest="genome_min_decoded_strata", metavar="mds", 
                                   help='Minimum decoded strata. Default to %d' % self.genome_min_decoded_strata)
        #-T threads_number
        mapping_group.add_argument('-T','--threads', dest="genome_threads", metavar="t", 
                                   help='Number of threads. Default to %d' % self.genome_threads)
        

    def register_mappingStats(self,parser):
        """Register stats parameter with the
        given arparse parser

        parser -- the argparse parser
        """
        stats_group = parser.add_argument_group('Mapping Stats')

        stats_group.add_argument('--no-stats', dest="mapping_stats_create", default=None, action="store_false", 
                                 help='Skip creating stats')
        stats_group.add_argument('--mappin-stats-json', dest="mapping_stats_json", default=None, action="store_true", 
                                 help='Write a json file with the statistics in addition to the normal stats output.')
                                 
    def register_output(self, parser):
        """Register all output parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        output_group = parser.add_argument_group('Output')
        output_group.add_argument('-n', '--name', dest="name", metavar="name", help="""Name used for the results. If not specified, the name is inferred from
            the input files""")
        output_group.add_argument('-o', '--output-dir', dest="output_dir", metavar="dir", help='Optional output folder. If not specified the current working directory is used.')
   

class CopyNumberCallingPipeline(BasePipeline):
    """Copy Number Calling Pipeline"""  
    def membersInitiation(self):
        #mrCanavar parameters
        self.conf_file = None  #Reference configuration file
        #Duplications calls
        self.duplications_create = True
        self.bed_repeat_regions = None #File of bed repeat regions
        self.bed_gaps_coordinates = None #File of bed gap coordinates
        #Documentation files
        self.generate_documentation = True #Not Create Documentation
        self.sample_description = None
        self.map_json_list = [] #List of mapping stats in case it exists Stats comming from GEM Mapping
        #Inputs compressed
        self.maps_gzipped = False #Flag to control if mappings are gzipped
  
    def referenceCheck(self,errors):
        """Reference checking, by default Checks gem index
        Parameters:
        errors - list of error messages"""
        if self.conf_file is None:
            errors.append("No mrCanavar Configuration file found")
        else:
            if not os.path.exists(self.conf_file):
                errors.append("mrCanavar Configuration not found: %s" % (self.conf_file))
            else:
                self.conf_file = os.path.abspath(self.conf_file)
    
    def noStandardParameterChecking(self,errors):
        """No Standard Parameters checking
        Parameters:
        errors - list of error messages
        """
        if len(self.map_json_list) > 0:
            aux_json = []
            for json in self.map_json_list:
                if not os.path.exists(json):
                    errors.append("json stats file not found: %s" % (json))
                else:
                    aux_json.append(os.path.abspath(json))
           
            self.map_json_list = aux_json  
    
    def mrCanavarRD(self, name, suffix=None, configuration=None, dependencies=None, final=False, description="Copy Number Calls through mrCaNaVar"):
        """ Compute Read Depth from sam files """
        step = MrCanavarRDStep(name, dependencies=dependencies, final=final, description=description)        
        config = dotdict()
        
        config.conf_file = self.conf_file
        config.maps_gzipped = self.maps_gzipped

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
              
    def mrCanavarCalls(self, name, suffix=None, configuration=None, dependencies=None, final=False, description="Copy Number Calls through mrCaNaVar"):
        """ Perform copynumber calls from read depth """
        step = MrCanavarCallsStep(name, dependencies=dependencies, final=final, description=description)        
        config = dotdict()
        
        config.conf_file = self.conf_file

        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def cnDistribution(self, name, suffix=None, configuration=None, dependencies=None, final=False, description="Copy Number Distribuition analysis"):
        """ Copy number distribution analysis """
        step = CopyNumberDistributionStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id

    def duplication(self,name, suffix=None, configuration=None, 
                    dependencies=None, final=False, description="Call duplictions through two methods based on st dev copy number"):
        """ Call of duplications according to standard deviation in copy number control regions """
        step = DuplicationStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()

        config.bed_repeat_regions = self.bed_repeat_regions
        config.bed_gaps_coordinates = self.bed_gaps_coordinates
        config.duplications_create = self.duplications_create
 
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id       
        
    def htmlDocumentation(self,name, suffix=None, configuration=None, 
                    dependencies=None, final=False, description="Create html and Json Report"):
        """ Creates HTML and JSON Documetation  """
        step = DocumentationStep(name, dependencies=dependencies, final=final, description=description)
        config = dotdict()
        
        config.sample_description = self.sample_description
        config.map_json_list = self.map_json_list
        
        if configuration is not None:
            self.__update_dict(config, configuration)

        step.prepare(len(self.steps), self, config)
        self.steps.append(step)
        return step.id
        
    def initialize(self, silent=False):
        # check general parameter
        errors = []
       
        if self.input is None:
            errors.append("No input dir specified")
        else:
            # check input files
            if self.input is None or not os.path.exists(self.input):
                errors.append("Input file not found: %s" % (self.input))
            else:
                # make aboslute path
                self.input = os.path.abspath(self.input)

        if self.name is None or len(self.name) == 0:
            errors.append("No name specified and unable to guess one. Please use --name to set a name explicitly.")

        self.referenceCheck(errors)
        
        if self.output_dir is None:
            self.output_dir = os.getcwd()

        self.output_dir = os.path.abspath(self.output_dir)

        self.noStandardParameterChecking(errors)

        if not silent and len(errors) > 0 and self.write_config is None:
            raise PipelineError("Failed to initialize neccessary parameters:\n\n%s" % ("\n".join(errors)))
        if self.write_config is not None:
            # log configuration errors
            logging.gemtools.warning("---------------------------------------------")
            logging.gemtools.warning("Writing configuration")
            logging.gemtools.warning("")
            logging.gemtools.warning("Note that some of the parameters are missing:\n")
            for e in errors:
                logging.gemtools.warning("\t" + str(e))
            logging.gemtools.warning("---------------------------------------------")
  
    def print_parameters(self,printer):
        """Print class parameters"""
        printer("MrCaNaVar conf file    : %s", self.conf_file) 
        printer("Call Duplications      : %s", self.duplications_create)
        printer("Bed Repats             : %s", self.bed_repeat_regions)
        printer("Bed Gaps               : %s", self.bed_gaps_coordinates)
        printer("Sample Description     : %s", self.sample_description)
        printer("Mappings Gzipped       : %s", self.maps_gzipped)
        printer("")

    def register_general(self, parser):
        """Register all general parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        input_group = parser.add_argument_group('Input')
        ## general pipeline paramters
        input_group.add_argument('-d', '--dir', dest="input", metavar="input",
            help='''Directory were are located SAM mapping files from an exhaustive mapping''')
            
    def register_class_parameters(self,parser):
        """Class Parameters Registration"""
        self.register_mrCanavar(parser)
        self.register_duplication(parser)
        self.register_htmlDocumentation(parser)
        
    def register_mrCanavar(self,parser):
        """Register mrCanavar parameter with the
        given argparse parser

        parser -- the argparse parser
        """
        
        mr_canavar_group = parser.add_argument_group('mrCanavar Stats')
        
        mr_canavar_group.add_argument('--conf_file', dest="conf_file", metavar="CONF_PATH",
                                      help='Path to the configuration file build for the working reference with mrCanavar PREP step')
                                      
        mr_canavar_group.add_argument('--gz', dest="maps_gzipped", default=False, action="store_true", 
                                 help='Indicates the SAM files are compressed in gzip format.') 


    def register_duplication(self,parser):
        """Register duplication calling parameters with the
        given argparser parser
        
        parser -- the argparse parser
        """

        call_duplications_group = parser.add_argument_group('Call Duplications')
        
        call_duplications_group.add_argument('--no-duplications', dest="duplications_create", default=None, action="store_false", 
                                 help='Skip calling duplications')                    
        call_duplications_group.add_argument('-bed-repeat-regions', dest="bed_repeat_regions", metavar="REPEATS_PATH", 
                                        help='''Bed file with repeat regions coordinates (include TRF, Simple Repeats amd kmer masking regions''')    
        call_duplications_group.add_argument('-bed-gaps-coordinates', dest="bed_gaps_coordinates", metavar="GAPS_PATH",
                                      help='Bed file with gaps coordinates')

    def register_htmlDocumentation(self,parser):
        """Register the documentation parameters with the
        given arparse parser

        parser -- the argparse parser
        """
        documentation_group = parser.add_argument_group('Documentation')      
        documentation_group.add_argument('--no-documentation', dest="generate_documentation", default=None, action="store_false", 
                                 help='Skip calling documentation') 
        documentation_group.add_argument('-sample-description',dest="sample_description",metavar="SAMPLE_DESCRIPTION",
                                         help='''Description text for the sample''')
                                         
        documentation_group.add_argument('-j', '--map-json', dest="map_json_list",nargs="+",
            help='''List of json files comming from mapping statistics.''')
        

    def register_output(self, parser):
        """Register all output parameters with the given
        argparse parser

        parser -- the argparse parser
        """
        output_group = parser.add_argument_group('Output')
        output_group.add_argument('-n', '--name', dest="name", metavar="name", help="""Name used for the results. If not specified, the name is inferred from
            the input files""")
        output_group.add_argument('-o', '--output-dir', dest="output_dir", metavar="dir", help='Optional output folder. If not specified the current working directory is used.')
   
 
